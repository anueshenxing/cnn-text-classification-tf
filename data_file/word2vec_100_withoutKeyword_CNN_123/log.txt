word2vec训练的词向量，词向量维度为100，训练集为85000×0.8条新闻数据，有键词，模型为LSTM+CNN
2017-03-20T21:25:54.086414: loss 2.34587, acc 0.12589
2017-03-20T21:27:38.120037: loss 2.25972, acc 0.195411
2017-03-20T21:29:20.485538: loss 2.17106, acc 0.290411
2017-03-20T21:31:08.977743: loss 2.08814, acc 0.363288
2017-03-20T21:33:00.701034: loss 2.00048, acc 0.455616
2017-03-20T21:34:41.934502: loss 1.90841, acc 0.569795
2017-03-20T21:36:30.818754: loss 1.81548, acc 0.605479
2017-03-20T21:38:29.190051: loss 1.72021, acc 0.643219
2017-03-20T21:40:43.785588: loss 1.62243, acc 0.686575
2017-03-20T21:43:19.288322: loss 1.52475, acc 0.716233
2017-03-20T21:46:36.288779: loss 1.42604, acc 0.760616
2017-03-20T21:49:42.219710: loss 1.33034, acc 0.775685
2017-03-20T21:52:58.576237: loss 1.23404, acc 0.796301
2017-03-20T21:56:23.694819: loss 1.14456, acc 0.812945
2017-03-20T21:59:06.684642: loss 1.06027, acc 0.832397
2017-03-20T22:01:46.631561: loss 0.983964, acc 0.832534
2017-03-20T22:03:31.724262: loss 0.913125, acc 0.844315
2017-03-20T22:05:13.403707: loss 0.848971, acc 0.847123
2017-03-20T22:06:55.166845: loss 0.792529, acc 0.853219
2017-03-20T22:08:35.480146: loss 0.738867, acc 0.858425
2017-03-20T22:10:15.528237: loss 0.692925, acc 0.860342
2017-03-20T22:11:56.224474: loss 0.65129, acc 0.866575
2017-03-20T22:13:36.383319: loss 0.616355, acc 0.865822
2017-03-20T22:15:16.809994: loss 0.583114, acc 0.871027
2017-03-20T22:16:56.815514: loss 0.554535, acc 0.875548
2017-03-20T22:18:37.818403: loss 0.530693, acc 0.873356
2017-03-20T22:20:18.769145: loss 0.507074, acc 0.878151
2017-03-20T22:21:58.942342: loss 0.48747, acc 0.87911
2017-03-20T22:23:39.613136: loss 0.470404, acc 0.879932
2017-03-20T22:25:19.820065: loss 0.454571, acc 0.881507
2017-03-20T22:26:59.922553: loss 0.439287, acc 0.884247
2017-03-20T22:28:40.186060: loss 0.42888, acc 0.884384
2017-03-20T22:30:21.032103: loss 0.416163, acc 0.887123
2017-03-20T22:32:01.665004: loss 0.405027, acc 0.888973
2017-03-20T22:33:41.887309: loss 0.39655, acc 0.888904
2017-03-20T22:35:22.418510: loss 0.387059, acc 0.891575
2017-03-20T22:37:01.459827: loss 0.380184, acc 0.891849
2017-03-20T22:38:41.164134: loss 0.372099, acc 0.893082
2017-03-20T22:40:20.220627: loss 0.367182, acc 0.892055
2017-03-20T22:41:59.791159: loss 0.360397, acc 0.894384
2017-03-20T22:43:38.624925: loss 0.354222, acc 0.895411
2017-03-20T22:45:17.427159: loss 0.349289, acc 0.896164
2017-03-20T22:46:56.728170: loss 0.345012, acc 0.896918
2017-03-20T22:48:36.198498: loss 0.340691, acc 0.898014
2017-03-20T22:50:16.945644: loss 0.336358, acc 0.898904
2017-03-20T22:51:57.056248: loss 0.332837, acc 0.899315
2017-03-20T22:53:36.880082: loss 0.32885, acc 0.899726
2017-03-20T22:55:16.331854: loss 0.325992, acc 0.900068
2017-03-20T22:56:55.414680: loss 0.322366, acc 0.901096
2017-03-20T22:58:34.763451: loss 0.32025, acc 0.901849
2017-03-20T23:00:14.647556: loss 0.318256, acc 0.902329
2017-03-20T23:01:54.909198: loss 0.314396, acc 0.901301
2017-03-20T23:03:35.582021: loss 0.311644, acc 0.902945
2017-03-20T23:05:16.624622: loss 0.309751, acc 0.902466
2017-03-20T23:06:56.791506: loss 0.307345, acc 0.903493
2017-03-20T23:08:37.209506: loss 0.305034, acc 0.904452
2017-03-20T23:10:16.786525: loss 0.30344, acc 0.905
2017-03-20T23:11:56.667997: loss 0.30214, acc 0.90589
2017-03-20T23:13:36.506089: loss 0.299845, acc 0.905685
2017-03-20T23:15:16.485134: loss 0.298317, acc 0.906301
2017-03-20T23:16:55.831475: loss 0.295833, acc 0.907123
2017-03-20T23:18:35.295502: loss 0.294491, acc 0.907123
2017-03-20T23:20:15.547232: loss 0.292656, acc 0.907192
2017-03-20T23:21:56.895457: loss 0.291121, acc 0.908014
2017-03-20T23:23:36.799705: loss 0.290727, acc 0.90774
2017-03-20T23:25:16.633330: loss 0.289483, acc 0.908699
2017-03-20T23:26:56.804950: loss 0.288372, acc 0.908425
2017-03-20T23:28:36.356647: loss 0.287487, acc 0.909041
2017-03-20T23:30:16.090561: loss 0.285986, acc 0.909863
2017-03-20T23:31:55.537210: loss 0.283791, acc 0.909932
